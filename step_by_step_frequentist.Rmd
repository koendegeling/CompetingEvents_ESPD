---
title: "Frequentist step-by-step implementation"
output: html_document
date: '2023-01-02'
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE)

## Libraries
require(maxLik)
```

## Maximum likelihood estimation (MLE): a brief reminder

Suppose we observe a sample of independent observations coming from the
same distribution, i.e. $X_i \sim \mathcal{F}$ for $i \in 1\dots N$.
Suppose further $\mathcal{F}$ is parametrised by vector $\theta$
governing the distribution, and has the corresponding density
$f(\cdot; \theta)$. The likelihood of the observed sample given $\theta$
is then

$$
\mathcal{L}_N(\theta; X) = \prod_{i=1}^N f(X_i;\theta)
$$

The goal of MLE is to find the optimal vector $\hat{\theta}$, namely

$$
\hat{\theta} := {\arg\max}_{\theta \in \Theta}\mathcal{L}_N(\theta; X) 
$$

where $\Theta$ is called the parameter space. In what follows, we will
use this approach to model various risk-related phenomena of increasing
complexity, taking us from a simple Weibull distribution to the mixture
of experts approach to dealing with competing risks.

### A simple case:

Below we show how to generate a Weibull variable given a vector of
parameters, and then use MLE to retrieve this parameters given the data.
Type `?rweibull` in the console for more details. For reproducibility,
we set the random number seed to `123` using the `set.seed()` function.

```{r}
set.seed(123)
n_sim <- 1000
times <- rweibull(n_sim, shape = 2.2, scale = 7.2)
hist(times, breaks = 30, col = "lightblue")
```

Defining the log-likelihood for this simple case -- given a vector of
parameters $\theta$, which are represented through the `theta` argument
of the `ll_weibull()` function below that is a vector of length 2:

\- Calculate the Weibull density for each data point with shape/scale
set to $(\theta_1,\theta_2)$

\- Sum the logs of the densities

```{r}
ll_weibull <- function(theta, times){
  d  <- dweibull(x=times, shape=theta[1], scale=theta[2])
  ll <- sum(log(d))
  return(ll)
}
```

We now perform maximisation of the `ll_weibull()` function given our
data using the `optim()` function, which is the built-in optimisation
function of `R`:

-   Set the start parameter values for the `theta` vector through the
    `par` argument
-   Define the objective function that is to be optimised using the `fn`
    argument
-   Specify the optimisation method using the `method` argument (if
    interested, start with `?optim` for an overview of available
    optimisers)
-   Because `optim()` minimises the objective function, we tell the
    function to multiply the objective by `-1` through the `fnscale`
    option in the `control` argument, which turns it in a maximisation
    problem
-   Since the `times` argument in the `ll_weibull()` function is not
    defined by the `optim()` function, we need to specify where the
    corresponding values can be found, which is the likely-named vector
    in this case
-   After completing the optimisation, the optimal parameters can be
    extracted from the object that is returned by the `optim()` function

```{r warning=FALSE}
pars <- optim(par=c(1,1), fn=ll_weibull, method="BFGS", control=list(fnscale=-1), times=times)
print(pars$par)
```

We see that obtained values are not quite the true parameters -- this is
normal, and becomes less pronounced as sample size increases. You can
convince yourself this is the case by playing with `n_sim` in the above.

### With censoring

Unfortunately, very often in real world datasets some of the outcomes
are censored because corresponding events fall outside the period of
observation. In order to simulate such scenario, we proceed as follows:

-   Define an independent exponential variable with a set rate (see
    `?rexp` for details)

-   Tune this parameter, which we defined as `c_rate`, to achieve the
    level of censoring required

-   These times correspond to the *"time left in the experiment"* for
    each data point

```{r}
c_rate <- 0.07
cens_times <- rexp(n = n_sim, rate = c_rate)
hist(cens_times, breaks = 25, col = "lightblue")
```

To determine whether an observation will be censored, the 'censoring
times' are compared with the actual simulated 'event times' --- each
element of the vector `c` below will be:

-   1 if the event time is lower than the censoring time and, hence, the
    event occurred

-   0 if the censoring time is lower than the even time and, hence, the
    observation is censored

```{r}
c <- ifelse(times <= cens_times, 1, 0)
print(paste0("Proportion of censored observations: ", (sum(c==0)/n_sim)*100, "%"))
```

Similarly, the actual observed times will be minimum of the event time
and the end of the experiment:

```{r}
t <- pmin(times, cens_times)
hist(t, breaks = 25, col = "lightblue")
```

The **likelihood** in the presence of censoring is different from the
one without censoring

-   If an event time $x_i = (t,c)$ is observed (i.e., $c=1$), then
    $\mathbf{P}(x_i|\theta) = w(t, \theta_k)$, where
    $w = \frac{d}{dt}\mathcal{W}$ is the density of the Weibull
    distribution

-   If an observation $x_i = (t,c)$ is censored (i.e., $c=0$), then we
    know that the event has not happened by that time, namely $$
      \mathbf{P}(x_i | \theta) = (1 - \mathcal{W}(t, \theta))
    $$

The previously defined `ll_weibull()` function can be adapted
accordingly to account for the presence of censoring:

-   An argument `cens` is added to the function to account for the
    censoring indicator

-   The `ifelse()` function is use to apply the density function if an
    event occurred (i.e., `cens==1`) or the survival function otherwise
    (i.e., `cens==0`)

```{r}
ll_weibull_cens <- function(theta, times, cens){
  l <- ifelse(cens==1, 
              dweibull(x=times, shape=theta[1], scale=theta[2]), 
              pweibull(q=times, shape=theta[1], scale=theta[2], lower.tail=FALSE))
  ll <- sum(log(l))
  return(ll)
}
```

Now the `optim()` function can be provide in a similar way as before,
but with the updated data vectors `t` and `c`, the latter of which is
used to define the `cens` argument of the `ll_weibull_cens()` function.

```{r warning=FALSE}
pars_cens <- optim(par=c(1,1), fn=ll_weibull_cens, method="BFGS", control=list(fnscale=-1), times=t, cens=c)
print(pars_cens$par)
```

### With censoring and covariates

When it comes to real world data, assuming that the distribution of the
event times does not depend on the individual is unrealistic. In other
words, if we have data on individual-level characteristics, we may want
to allow for the possibility that the estimated parameters are informed
by such characteristics.

To that end, we incorporate covariates in the model using the following
parametrisation:

$$
f(y_i|\alpha,\sigma_i) =\frac{\alpha}{\sigma_i}\left(\frac{y_i}{\sigma_i}\right)^{\alpha-1}e^{-(y_i/\sigma_i)^{\alpha}}
$$where

$$
\sigma_{i} = \exp{\left( - \frac{\mu + X_{i}^{T}\beta}{\alpha} \right)}
$$

is the scale, which now includes participant's (baseline) information,
such as sex, age, etc. We define new functions for sampling from a
Weibull distribution based on covariate information (i.e.,
`rweibull_cov()`), the density function (i.e., `dweibull_cov()`) and the
cumulative probability function (i.e., `pweibull_cov()`), according to
the above:

```{r}
rweibull_cov <- function(alpha, mu, beta, x) {
    n <- length(x)
    sig <- exp(-(mu + beta %*% x) / alpha)
    return(rweibull(n = n, shape = alpha, scale = sig))
}

dweibull_cov <- function(t, alpha, mu, beta, x) {
    sig <- exp(-(mu + beta %*% x) / alpha)
    return(dweibull(t, shape = alpha, scale = sig))
}

pweibull_cov <- function(t, alpha, mu, beta, x, lower.tail = TRUE) {
    sig <- exp(-(mu + beta %*% x) / alpha)
    return(pweibull(t, shape = alpha, scale = sig, lower.tail = lower.tail))
}
```

We now generate covariate "age", normalized at a mean value of zero, set
its effect size (`beta_age`), and sample corresponding event times:

```{r}
set.seed(123)
n_sim <- 100
age <- rnorm(n = n_sim, mean = 0, sd = 1)

alpha <- 2
mu <- -4
beta_age <- 1.5
times <- rweibull_cov(alpha, mu, beta_age, age)

hist(times, breaks = 30, col = "lightblue")
```

Censoring is performed as before, as well as subsequent steps of
defining vectors `t` and `c`:

```{r}
c_rate <- 0.1
cens_times <- rexp(n = n_sim, rate = c_rate)
c <- ifelse(times <= cens_times, 1, 0)
t <- pmin(times, cens_times)
print(paste0("Proportion of censored observations: ", (sum(c==0)/n_sim)*100, "%"))
```

Of course, we should also adjust the code for the likelihood accordingly
--- it looks similar to the `ll_weibull_cens_cov()` function, but now
the density and probability functions include the covariates vector `X`
and the custom functions that accommodate for the the covariates:

```{r}
ll_weibull_cens_cov <- function(theta, times, cens, X){
  l <- ifelse(cens==1, 
              dweibull_cov(t=times, alpha=theta[1], mu=theta[2], beta=theta[3], x=X), 
              pweibull_cov(t=times, alpha=theta[1], mu=theta[2], beta=theta[3], x=X, lower.tail=FALSE))
  ll <- sum(log(l))
  return(ll)
}
```

Similarly, the call of the `optim()` functions needs to be updated to
provide a start value for an additional parameter and to provide the
`age` vector as value for argument `X`:

```{r warning=FALSE}
pars_cens_cov <- optim(par=c(1,0,0), fn=ll_weibull_cens_cov, method="BFGS", control=list(fnscale=-1), times=t, cens=c, X=age)
print(pars_cens_cov$par)
```

### One risk is good -- two risks are more realistic

We are now very close --- one thing we need to learn how to do is
dealing with multiple competing risks. In line with the definition of
the event-specific probabilities and distributions (ESPD) modelling
approach (see paper for details), we do the following:

1.  Pick which event will happen according to mixture proportions

2.  Sample time to event given parameters of the chosen risk

This is illustrated in the code below:

-   Two sets of parameters are now defined, i.e. `theta1` and `theta2`,
    one for each competing event

-   The probability that competing event 1 occurs, i.e. `prob1` is
    defined to be 30%

-   For illustration purposes, covariates are ignored for now

```{r}
set.seed(123)
n_sim <- 1000
theta1 <- c(3, 2)
theta2 <- c(5, 5)
prob1 <- c(0.3)

times1 <- rweibull(n_sim, shape = theta1[1], scale = theta1[2])
times2 <- rweibull(n_sim, shape = theta2[1], scale = theta2[2])

events <- sample(x = c(1,2), size = n_sim, prob = c(prob1, 1-prob1), replace = TRUE)
times <- ifelse(events==1, times1, times2)

hist(times, breaks = 30, col = "lightblue")
```

Notice above that, as expected, the distribution is bi-modal (two risks
considered)! Next, censoring is applied as before, with the exception
that there are now two types of events that can occur in addition to
censoring. Hence, we rename the censoring vector `c` to event vector
`e`, with values: 0 for censoring, 1 for event 1, and 2 for event 2:

```{r}
c_rate <- 0.1
cens_times <- rexp(n = n_sim, rate = c_rate)
e <- ifelse(times <= cens_times, events, 0)
t <- pmin(times, cens_times)
print(paste0("Proportion of censored observations: ", (sum(e==0)/n_sim)*100, "%"))
```

In defining the likelihood function for the mixture, the vector of
mixture proportions or event probabilities, $\pi$, describes a
probability distribution. In other words, all its elements are
non-negative and sum to one. This makes what follows a constrained
optimisation problem. In order to avoid dealing with resulting
complications, we use a *sigmoid* function to transform the
probabilities to log-odds ratios, like in logistic regression modelling:

```{r}
sigmoid <- function(x){
  return(1/(1+exp(-x)))
}
```

A similar procedure is often applied to parameters of the distributions
like the Weibull distribution in this example. By modelling the shape
and scale parameter on log-scale, it can be assured their values are
non-negative according to their definition. This is also implemented in
the general function that will be introduced later for modelling
competing events according to the ESPD approach.

Based on the above, the likelihood is defined below. The `theta` vector
now contains 5 parameters: 1 for the probability of event 1, 2 for the
Weibull distribution for event 1, and 2 for the Weibull distribution for
event 2. Note that there are now two events in addition to the censoring
status. Rather than using nested if-else statements, the code below
illustrates how to us a power-calculation to set the values to 1 if the
event did not occur, or to their respective value if th event did occur.
For example, the term `(events==1)` results in a vector of ones for
those who experienced event 1, and zeros for those for whom the event
was not 1. The result is that for those who experiences event 1, the
density applies, whereas for the others a value of 1 will be returned,
i.e. they are not considered in the likelihood for event 1.

```{r}
ll_weibull_mix <- function(theta, times, events){
  p1 <- sigmoid(theta[1])
  l1 <- (p1*dweibull(times, theta[2], theta[3]))^(events==1)
  l2 <- ((1-p1)*dweibull(times, theta[4], theta[5]))^(events==2)
  l0 <- (p1*pweibull(times, theta[2], theta[3], FALSE) + (1-p1)*pweibull(times, theta[4], theta[5], FALSE))^(events==0)
  ll <- sum(log(l1) + log(l2) + log(l0))
  return(ll)
}
```

After defining the likelihood function, the call of the `optim()`
functions needs to be updated to provide start values for all 5
parameters, and to provide the `events` argument:

```{r warning=FALSE}
pars_mix_cens <- optim(par=c(0,1,1,1,1), fn=ll_weibull_mix, method="BFGS", control=list(fnscale=-1), times=t, events=e)

print(sigmoid(pars_mix_cens$par[1]))
print(pars_mix_cens$par[-1])

```

### Applying it to the case study

We are now in a position to put everything we have learned so far to
practical use. We proceed as follows (also see the manuscript for an
introduction to the case study and further context):

-   Load the `melanoma` dataset from the `boot` package and wrangle it
    to correspond to the required format

-   Define covariates for both death and progression risk processes, as
    well as mixture proportions

-   Define the `ll_weibull_mix_cov` in order to find the optimal
    parameter values by maximum likelihood

First, we load the `melanoma` dataset from the `boot` package and define
the competing events:

```{r}
# Loading the melanoma dataset
mel_df <- boot::melanoma

# Transforming time from days to years
times <- mel_df$time / 365.25

# Defining the event indicator based on the status variable in the datases:
# - status = 1 corresponds to death      
# - status = 2 corresponds to recurrence 
# - status = 3 corresponds to censoring  
events <- ifelse(mel_df$status==1, 'death', ifelse(mel_df$status==2, 'recur', 'cens'))
```

Next, we define the covariate matrix `X` including the age, sex,
ulceration status, and tumor thickness for each individual (note that we
scale the age and thickness variables using the `scale()` function that
is part of the standard installation of `R`):

```{r}
# Construct the matrix of covariates, adding the incercept column filled with 1's
X <- cbind(1, scale(mel_df$age), mel_df$sex, mel_df$ulcer, scale(mel_df$thickness))

# Set the column names accordingly
colnames(X) <- c('intercept', 'age', 'sex', 'ulcer', 'thickness')

```

Combining all the above, we can define a custom log-likelihood function
for this case study as introduced and explained in the manuscript:

```{r}
ll_weibull_mix_cov <- function(coefs, times, events, X){
  
  # Extract the coefficients for the different parameters from the vector
  coefs_logit_recur <- coefs[1:5]
  coefs_shape_recur <- coefs[6]
  coefs_scale_recur <- coefs[7:11]
  coefs_shape_death <- coefs[12]
  coefs_scale_death <- coefs[13:17]

  # Obtain the patient-level parameters based on the patient covariates 
  # and the extracted parameter coefficients
  p_recur     <- sigmoid(coefs_logit_recur)
  shape_recur <- exp(coefs_shape_recur)  
  scale_recur <- exp(X %*% coefs_scale_recur)
  shape_death <- exp(coefs_shape_death)
  scale_death <- exp(X %*% coefs_scale_death)

  # Log likelihood for those who recurred 
  ll_recur <- p_recur * dweibull(times, shape_recur, scale_recur)^(events == 'recur')

  # Log likelihood for those who deceased
  ll_death <- (1-p_recur) * dweibull(times, shape_death, scale_death)^(events == 'death')

  # Log likelihood for censored event
  ll_cens <- (p_recur * pweibull(times, shape_recur, scale_recur, FALSE) +
                  (1-p_recur) * pweibull(times, shape_death, scale_death, FALSE))^(events == 'cens')
  
  # Combine the log likelihoods
  ll <- sum(log(ll_recur) + log(ll_death) + log(ll_cens))

  return(ll)

}

```

We can now perform the optimization using the `optim()` function.
Additionally, the code below demonstrates how it can be performed using
the `maxLik()` function from the `maxLim` package, which was developed
for this exact purpose and has some benefits like returning the
variance-covariance matrix, as discussed in the manuscript. Different
than for previous illustrations above, we first define the start values
for the parameters and give them names, so that the results can be
interpreted more easily:

```{r warning=FALSE}

# Define the start values for the coefficients and name them
start_values <- setNames(
  object = rep(x = 0, times = 17),
  nm = c('logit_recur_intercept', 'logit_recur_age', 'logit_recur_ sex', 
       'logit_recur_ ulceration', 'logit_recur_thickness', 'logshape_recur',
       'logscale_recur_intercept', 'logscale_recur_age', 
       'logscale_recur_ sex', 'logscale _recur_ ulceration', 
       'logscale_recur_thickness', 'logshape_death', 
       'logscale_death_intercept', 'logscale_death_age', 
       'logscale_death_ sex', 'logscale_death_ ulceration', 
       'logscale_death_thickness')
)

# Perform the likelihood optimization using the optim() function
pars_optim <- optim(par=start_values, fn=ll_weibull_mix_cov, method="BFGS", control=list(fnscale=-1), times=times, events=events, X=X)

# Perform the likelihood optimization using the maxLik() function
pars_maxLik <- maxLik(
  logLik = ll_weibull_mix_cov,
  start = start_values,
  times = times,
  events = events,
  X = X
)

# Extract and compare the coefficients
cbind(optim = pars_optim$par, maxLik = coef(pars_maxLik))

```

As you can see and as expected, the estimates of the parameters are very
similar regardless of the optimisation function used. The estimated
parameters can now be used in a discrete event simulation to model the
competing events of recurrence and death.

### What next?

In this tutorial we showed how to perform maximum likelihood estimation
for the case of competing risks from scratch. However, this is not the
end of the road -- in reality one may require quite a bit more
flexibility, such as allowing for different parametric families of
distributions for individual risks, as well goodness-of-fit diagnostics
such as information criteria.

Having mastered material in this notebook, you are well-positioned to
develop all that functionality yourself, but thankfully we have already
done it so that you don't have to!

The `ESPD_frequentist.R` script contains a general function that has all
the functionality you may ever require, and used the `maxLik` package
for the maximum likelihood estimation so that the variance-covariance
matrix is obtained as well. The above material should be sufficient for
anyone to get a good grasp of what is happening under the hood of this
function through the extensive comments that it includes. We look
forward to you trying it out and letting us know what you think!
