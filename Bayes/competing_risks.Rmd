---
title: "Competing risks"
output: 
  html_notebook:
    code_folding: hide
---
Load some libraries... 
```{r message=FALSE, warning=FALSE}
library(data.table)
library(cmdstanr)
# Making sure your Stan models run in parallel 
options(mc.cores = parallel::detectCores() - 1)
```
Generate the data: 
```{r}
# This could go much larger, but for 200K yuo'd be waiting for Stan to finish for a fairly long time :)  
n_data <- 1000

p_event1 <- 0.35
d_event1 <- c(shape = 2, scale = 10)
d_event2 <- c(shape = 5, scale = 5)

# For the data set that is simulated according to the ESPD approach, first the 
# event is sampled and, conditionally, the time-to-event is sampled
# - note that the events are identified as 1 and 2
df_ESPD <- data.frame(
  event = ifelse(runif(n_data) < p_event1, 1L, 2L)
)

df_ESPD$time <- ifelse(df_ESPD$event == 1L,
                       rweibull(n_data, d_event1['shape'], d_event1['scale']),
                       rweibull(n_data, d_event2['shape'], d_event2['scale']))

# Censoring, if to be included, is identified as a 0 and the time of censoring
# is a random time between 0 and the time at which the event would have happened
p_censor <- 0.4

df_ESPD$cens <- ifelse(runif(n_data) < p_censor, 0L, df_ESPD$event)
df_ESPD$time <- ifelse(df_ESPD$cens == 0L, runif(n_data, 0, df_ESPD$time), df_ESPD$time)
```
Likelihood function for the data generated (we shall see it's not the right one, but here for historical reasons)

```{r}
logL_ESPD_cust <- function(x, times, events) {
  
  d_event1 <- c(shape = exp(x[1]), scale = exp(x[2]))
  d_event2 <- c(shape = exp(x[3]), scale = exp(x[4]))
  
  c1 <- (events == 1L)
  c2 <- (events == 2L)
  
  f1_event1 <- dweibull(times[c1], d_event1['shape'], d_event1['scale'])
  f2_event2 <- dweibull(times[c2], d_event2['shape'], d_event2['scale'])
  
  F1_event0 <- pweibull(times[!c1 & !c2], d_event1['shape'], d_event1['scale'])
  F2_event0 <- pweibull(times[!c1 & !c2], d_event2['shape'], d_event2['scale'])
  
  logL <- sum(log(f1_event1), log(f2_event2), log(1-(F1_event0*F2_event0)))
  print(paste0("The last term: ", sum(log(1-(F1_event0*F2_event0)))))
  return(logL)
}

ESPDfit_ESPDdata_TTE_cust <- optim(par = log(c(1, 7, 1, 7)), fn = logL_ESPD_cust, control = list(fnscale = -1), 
                                   times = df_ESPD$time, events = df_ESPD$cens)

# So we can see the original parameters are almost perfectly estimated
exp(ESPDfit_ESPDdata_TTE_cust$par) 
c(d_event1, d_event2)
```
First let's address the likelihood above -- shows that the last term is largely irrelevant since $F_1(\cdot)F_2(\cdot)$ is usually small
```{r}
x1 = log(c(1, 7, 1, 7))
x2 = log(c(2, 10, 5, 5))
l1 = logL_ESPD_cust(x1, df_ESPD$time, df_ESPD$cens)
l2 = logL_ESPD_cust(x2, df_ESPD$time, df_ESPD$cens)
```

## The model 

### The actual data-generating process: 
 
* Sample the event type $D$: $\mathbf{P}(D = k) = \pi_k, \sum_{k=1}^K \pi_k = 1$, where $K$ is the number of competing risks 
  - You can either stipulate these proportions, or estimate them from the data -- if the amount of censoring is quite high, and data is not abundant, the latter is definitely preferable 
* Sample time to event $T$ from the corresponding distribution: $\mathbf{P}(T < t|D = k) = \mathcal{W}(t, \theta_k)$, where $\theta_k$ is the vector of parameters associated with that event type, and $\mathcal{W}$ is Weibull cdf
* Problems:
  - The way you censor is somewhat counter-intuitive, since in real world censoring is more likely the longer no event happens, while here it is independent from the time generated. For extra considerations regarding censoring see below :) 
  - This is naturally step one in the pipeline, but to make this reasonable one would need proportions to depend on some covariates (age, sex, previous recurrences,...)

### The priors: 
This is really up to you, but for the sake of simplicity I put $\text{Gamma}(0.1,0.1)$ priors on both scale and shape for all competing risks -- it is really weak (std is 10), here as a placeholder. Since we don't have covariates just yet, this is all we need

### The likelihood: 
Denoting $\theta = (\theta_1,\dots,\theta_K)$, we go over the data points $\{x_i\}_{i = 1}^N$

* If we see an entry $x_i = (t, k)$, then $\mathbf{P}(x_i|\theta) = \pi_k w(t, \theta_k)$, where $w = \frac{d}{dt}\mathcal{W}$ is the density of the corresponding Weibull distribution 
* If an observation $x_i = (t,c)$ is censored, then we know that whichever event was selected happened later, namely 
$$
  \mathbf{P}(x_i | \theta) = \sum_{k=1}^{K} \pi_k (1 - \mathcal{W}(t, \theta_k))
$$

### The code: 
Stan code for this model -- you can obviously just change your likelihood, but this allows you to quantify uncertainty in a principled manner, as well as modify the model in any way you want in a streamlined fashion:
```{r}
weibull_compete_code <- "
data {
  // dimensions
  int<lower=0> N;             // number of observations
  int<lower=1> D;             // number of competing risks 
  vector[N] y;                // time for observation n
  int<lower=0> event[N];      // event status (1:D -- event type, 0:censor) for obs n
}
parameters {
  simplex[D] theta;
  vector<lower=0>[D] alpha;
  vector<lower=0>[D] scale;
}
model {
  // priors
  vector[D] log_theta = log(theta);
  alpha ~ gamma(1e-1, 1e-1);
  scale ~ gamma(1e-1, 1e-1);

  // likelihood
  for (n in 1:N) {
      if (event[n] != 0){
          y[n] ~ weibull(alpha[event[n]], scale[event[n]]);
          target += log_theta[event[n]];
      }
      else {
        vector[D] lps = rep_vector(0, D);
        for (d in 1:D){
          lps[d] = log_theta[d] + weibull_lccdf(y[n] | alpha[d], scale[d]);
        }
        target += log_sum_exp(lps);
      }
  }
}

"
mod.cr = cmdstan_model(stan_file = write_stan_file(weibull_compete_code))
```
Once you've fitted this model to data you have, you can use *generated quantities* block in Stan to simulate a synthetic cohort by sampling from posteriors. Woo-hoo

```{r message=FALSE, warning=FALSE}

dat.stan <- list(
  N = nrow(df_ESPD),
  D = length(unique(df_ESPD$event)),
  event = df_ESPD$cens,
  y = df_ESPD$time
)
fit.cr <- mod.cr$sample(
  data = dat.stan,
  #seed = 123,
  chains = 4,
  parallel_chains = getOption("mc.cores", 1),
  refresh = 25,
  max_treedepth = 10,
  adapt_delta = 0.99,
  iter_warmup = 1000,
  iter_sampling = 1000
)
```
Let's see the results of out fit: 
```{r}
fit.cr$summary()
```


```{r}
bayesplot::color_scheme_set("red")
# bayesplot::mcmc_areas(fit.cr$draws(c("scale", "alpha"))) + ggplot2::ggtitle("Posterior distributions",
#                       "with medians and 80% intervals")
bayesplot::mcmc_hist(fit.cr$draws(c("scale", "alpha", "theta")), stat = "mean") #+ bayesplot::vline_at(apply(fit.cr$draws(c("scale", "alpha", "theta")), 3, mean))
```
This is very close, but not quite there -- why? The answer is the way you censor the data. If an observation is censored, we sample a uniform variable, not the other way around (the time we end up seeing is very much dependent on the original time sampled). In other words, denoting the censored time we see as $T$, and assuming the original event was of type $k$, we have 
$$
  P(T \in dt | D = k) = \int_t^{\infty} P(T \in dt | T_k \in ds)P(T_k \in ds) = \bigg[\int_t^{\infty} \frac{w(s, \theta_k)}{s}ds \bigg]dt \not\propto (1 - \mathcal{W}(t, \theta_k))dt
$$
and we see how the likelihood is misspecified. Let's try a different method: 

* Generate data as before
* Generate an exponential RV with a particular rate so that the fraction of censoring is to our satisfaction -- you can make this precise, but here I just eyeball and see

```{r}
n_data <- 5000

p_event1 <- 0.35
d_event1 <- c(shape = 2, scale = 10)
d_event2 <- c(shape = 5, scale = 5)

c.rate = 0.1

df <- data.table(
  event = ifelse(runif(n_data) < p_event1, 1L, 2L)
)

df$time <- ifelse(df$event == 1L,
                       rweibull(n_data, d_event1['shape'], d_event1['scale']),
                       rweibull(n_data, d_event2['shape'], d_event2['scale']))
df$cens.times = rexp(n = n_data, rate = c.rate)

df[,(c("cens", "time")) := list(ifelse(time <= cens.times, event, 0), ifelse(time <= cens.times, time, cens.times))]

print(paste0("The rate of censoring: ", sum(df$cens == 0)/n_data))

```
We now re-run the same model, but in this case we actually fully captured the data-generating process -- when we see a censored time $t$, all we know is that the actual time occurred later:

```{r}
dat.stan <- list(
  N = nrow(df),
  D = length(unique(df$event)),
  event = df$cens,
  y = df$time
)
fit.cr.new <- mod.cr$sample(
  data = dat.stan,
  #seed = 123,
  chains = 4,
  parallel_chains = getOption("mc.cores", 1),
  refresh = 25,
  max_treedepth = 10,
  adapt_delta = 0.99,
  iter_warmup = 1000,
  iter_sampling = 1000
)
```
Summary: 
```{r}
fit.cr.new$summary()
```
An the mandatory posterior plots:
```{r}
bayesplot::mcmc_hist(fit.cr.new$draws(c("scale", "alpha", "theta")))
```
```{r}
dat.stan.new <- list(
  N = nrow(df_ESPD),
  D = length(unique(df_ESPD$event)),
  event = df_ESPD$cens,
  y = df_ESPD$time,
  Ncens = sum(df_ESPD$cens == 0)
)
mod.new <- cmdstan_model(stan_file = file.path("weib.compete.stan"))
init_list <- list(
  list(t_cens = matrix(2*max(df_ESPD$time), ncol = length(unique(df_ESPD$event)), nrow = sum(df_ESPD$cens == 0))),
  list(t_cens = matrix(2*max(df_ESPD$time), ncol = length(unique(df_ESPD$event)), nrow = sum(df_ESPD$cens == 0))),
  list(t_cens = matrix(2*max(df_ESPD$time), ncol = length(unique(df_ESPD$event)), nrow = sum(df_ESPD$cens == 0))),
  list(t_cens = matrix(2*max(df_ESPD$time), ncol = length(unique(df_ESPD$event)), nrow = sum(df_ESPD$cens == 0)))
)
fit.new <- mod.new$sample(
  data = dat.stan.new,
  init = init_list,
  chains = 4,
  parallel_chains = getOption("mc.cores", 1),
  refresh = 25,
  max_treedepth = 16,
  adapt_delta = 0.99,
  iter_warmup = 1000,
  iter_sampling = 1000
)

```

